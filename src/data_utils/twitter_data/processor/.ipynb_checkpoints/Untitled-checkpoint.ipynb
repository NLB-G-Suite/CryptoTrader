{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.sentiment import vader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'libs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-31e050f371c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlibs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriting_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_locations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'libs'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from libs.writing_utils import get_locations\n",
    "\n",
    "sys.path.append(os.path.dirname(get_locations()[1]))\n",
    "\n",
    "from common_modules.common_utils import trends_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('vader_lexicon')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../twitterscraper/tests/data/tweet/bitcoin/historic_scrape/raw/2015-02-01_2015-03-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(data):\n",
    "    pattern = [ 'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs'\n",
    "                '([^a-zA-Z0-9 ]+?)', #anything else except text\n",
    "                ]\n",
    "\n",
    "    sub_pattern = re.compile('|'.join(pattern))\n",
    "    common_w = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    if isinstance(data, pd.Series):\n",
    "        \n",
    "        data = data.str.lower()\n",
    "        \n",
    "        for idx, row in enumerate(data):\n",
    "            splitRow = row.split(' ')\n",
    "            splitRow = [w for w in splitRow if not w in common_w] \n",
    "\n",
    "            data.iloc[idx] = \" \".join(splitRow)\n",
    "            \n",
    "        replaced = data.str.replace(sub_pattern, '').str.strip()\n",
    "    else:\n",
    "\n",
    "        data = data.lower()\n",
    "        splitted = data.split(' ')\n",
    "\n",
    "        newSplit = [w for w in splitted if not w in common_w] \n",
    "        word = \" \".join(newSplit)\n",
    "        replaced = re.sub(sub_pattern, '', word).strip()\n",
    "        \n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = cleanData(df['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Tweet'].apply(lambda x: vader.SentimentIntensityAnalyzer().polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'] = pd.to_datetime(df['Time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = pd.Series()\n",
    "    \n",
    "    x = x.assign(f = x['Likes'] + x['Replies'] + x['Retweets']).sort_values('f', ascending=False).drop('f', axis=1)\n",
    "    x['sentiment'] = x['Tweet'].apply(lambda x: vader.SentimentIntensityAnalyzer().polarity_scores(x)['compound'])\n",
    "    \n",
    "    Nbullish = sum(x['sentiment'] > 0) #should use something else later\n",
    "    Nbearish = sum(x['sentiment'] < 0)\n",
    "    \n",
    "    y['mean_vader_all'] = x['sentiment'].mean()\n",
    "    \n",
    "    #doi.org/10.1016/j.eswa.2016.12.036\n",
    "    y['n_bullish_all'] = Nbullish\n",
    "    y['n_bearish_all'] = Nbearish\n",
    "    \n",
    "    try:\n",
    "        y['bullish_ratio_all'] = Nbullish / (Nbullish + Nbearish)\n",
    "    except:\n",
    "        y['bullish_ratio_all'] = np.nan\n",
    "    \n",
    "    try:\n",
    "        y['bearish_ratio_all'] = Nbearish / (Nbullish + Nbearish)\n",
    "    except:\n",
    "        y['bearish_ratio_all'] = np.nan\n",
    "        \n",
    "    y['bullish_index_all'] = np.log((Nbullish + 1)/(Nbearish + 1))\n",
    "    \n",
    "    try:\n",
    "        y['agreement_all'] = 1 - np.sqrt(1-(((Nbullish - Nbearish)/(Nbullish + Nbearish))**2) )\n",
    "    except:\n",
    "        y['agreement_all'] = np.nan\n",
    "        \n",
    "    try:\n",
    "        y['spread_all'] = (Nbullish - Nbearish)/(Nbullish + Nbearish)\n",
    "    except:\n",
    "        y['spread_all'] = np.nan\n",
    "    \n",
    "    xTopAll = x.iloc[:int(x.shape[0] * .15)]\n",
    "    \n",
    "    y['mean_vader_top'] = xTopAll['sentiment'].mean()\n",
    "    y['mean_likes_top'] = xTopAll['Likes'].mean()\n",
    "    y['mean_replies_top'] = xTopAll['Replies'].mean()\n",
    "    y['mean_retweets_top'] = xTopAll['Retweets'].mean()\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf = df.groupby(pd.Grouper(freq='H')).apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf['variation_all'] = newDf['n_bullish_all'].diff()\n",
    "newDf = newDf.drop(['n_bullish_all', 'n_bearish_all'], axis=1)\n",
    "newDf['mean_vader_change_top'] = newDf['mean_vader_top'].diff()\n",
    "#add botorNot too\n",
    "newDf = newDf.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_adder import trends_ta function and then apply ta to mean_vader_top and mean_vader_all\n",
    "newDf = trends_ta(newDf, 'mean_vader_top')\n",
    "newDf = trends_ta(newDf, 'mean_vader_all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
