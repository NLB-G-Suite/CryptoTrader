{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profilescraper import query_profile\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from proxy_utils import proxy_dict, get_proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiles_to_pandas(profiles):\n",
    "    userDf = pd.DataFrame(columns=['username', 'location', 'has_location', 'age', 'is_verified', 'total_tweets', 'total_following', 'total_followers', 'total_likes', 'total_moments', 'total_lists', 'has_avatar', 'has_background', 'is_protected', 'profile_modified', 'tweets'])\n",
    "    tweetDf = pd.DataFrame(columns=['User', 'ID', 'Tweet', 'Time', 'Likes', 'Replies', 'Retweet'])\n",
    "\n",
    "    for profile in profiles:   \n",
    "        for tweet in profile.tweets:\n",
    "            tweetDf = tweetDf.append({'User': profile.username, 'ID': tweet.id, 'Tweet': tweet.text, 'Time': tweet.timestamp, 'Likes': tweet.likes, 'Replies': tweet.replies, 'Retweet': tweet.retweets}, ignore_index=True)\n",
    "\n",
    "        userDf = userDf.append({'username':profile.username, 'location':profile.location, 'has_location':profile.has_location, 'age':profile.age, 'is_verified':profile.is_verified, 'total_tweets':profile.total_tweets, 'total_following':profile.total_following, 'total_followers':profile.total_followers, 'total_likes':profile.total_likes, 'total_moments':profile.total_moments, 'total_lists':profile.total_lists, 'has_avatar':profile.has_avatar, 'has_background':profile.has_background, 'is_protected':profile.is_protected, 'profile_modified':profile.profile_modified}, ignore_index=True)\n",
    "\n",
    "    tweetDf = tweetDf.to_csv('profiledata/userTweets.csv', index=None)\n",
    "    userDf['username'].to_csv('profiledata/extractedUsers.csv', index=None, mode='a')\n",
    "    userDf.to_csv('profiledata/userData.csv', index=None, mode='a')\n",
    "    \n",
    "    print(\"Saved to userTweets.csv and extractedUsers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_list(currList, poolsize, proxy, count):\n",
    "        \n",
    "    if (len(currList) > 0):\n",
    "        profiles = query_profile(currList, poolsize=poolsize, proxy=proxy)\n",
    "        profiles_to_pandas(profiles)\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_extraction(coinname, poolsize=20):\n",
    "    proxies = get_proxies()\n",
    "    proxySize = len(proxies)\n",
    "    \n",
    "    users = list(set(pd.read_csv('{}/extracted/combined.csv'.format(coinname), dtype=str)['User']))\n",
    "    \n",
    "    try:\n",
    "        alreadyRead = pd.read_csv('profiledata/extractedUsers.csv', header=None)[0]\n",
    "    except FileNotFoundError:\n",
    "        logging.info(\"Already extracted users not found - Starting from a clean slate\")\n",
    "        os.mknod(\"profiledata/extractedUsers.csv\")\n",
    "        alreadyRead = pd.Series()\n",
    "        \n",
    "    \n",
    "    uniqueUsers = list(set(users) - set(alreadyRead))\n",
    "    \n",
    "    print(\"File contains {} data. Scraping for {} after cache\".format(len(users), len(uniqueUsers)))\n",
    "    \n",
    "    oldi = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(0, len(uniqueUsers), poolsize*5):\n",
    "        count = scrape_list(uniqueUsers[oldi:i], poolsize=poolsize, proxy=proxies[count], count=count)\n",
    "        \n",
    "        if (count >= proxySize):\n",
    "            count = 0\n",
    "        \n",
    "        logging.info(\"Done {} of {}\".format(i, len(uniqueUsers)))\n",
    "        oldi = i\n",
    "    \n",
    "    scrape_list(uniqueUsers[i:], poolsize=poolsize, proxy=None, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ripple\n",
      "dogecoin\n",
      "monero\n",
      "stellar\n",
      "litecoin\n",
      "ethereum\n",
      "dash\n",
      "bitcoin\n"
     ]
    }
   ],
   "source": [
    "for files in glob(os.getcwd() + \"/*\"):\n",
    "    if (os.path.exists(files + \"/extracted\")):\n",
    "        perform_extraction(files.split(\"/\")[-1], poolsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
